---
title: 原始GNN和GCN网络详解
date: 2020-09-10 00:12:07
tags:
- 原始GNN
- GCN
---
# 0.概述
之前的博客中学习了神经网络，但是，神经网络只能处理简单的类似于表格的数据，其中没有各个数据之间的联系，例如图片数据等。
但是在实际中，我们需要表示各个数据之间的联系，所以我们引入了图结构，图中的节点表示数据，节点之间的线表示各个数据之间的关系，图结构能够表达出比表数据更多的内容。
下面是图结构数据的图像表示:
![](图结构数据图像.png)

这部分内容为:原始GNN网络和GCN网络



# 1.原始GNN网络

为了方便理解，我们需要建立如下的变量:
![](原始GNN网络变量.png)
其中，获取的数据中有每个节点的特征向量

简单解释一下原始GNN网络的结构。
1.对每个节点建立一个状态变量，每个节点的状态变量都取决于该节点的特征、和该节点相连接的边的特征、和该节点相连节点的特征、和该节点相邻节点的状态，对此建立一个状态转换方程
2.对每一个节点建立一个输出变量，每个节点的输出变量取决于该节点的状态、该节点的特征，对此建立一个输出方程
3.建立一个二次损失函数，使得其最小

上面两个方程中:
输出方程为线性方程，其中的系数都是需要训练的参数，初始时使用符合正态分布的随机数，
状态转换方程比较复杂，因为状态变量需要有解，也就是能够全局收敛，需要满足Banach不动点理论，为此，状态转换方程需要满足一定的条件，具体的条件下面继续说明。
在之后的训练中会对这些参数进行训练，使得二次损失函数越来越小，输出越来越接近我们期望的输出。

训练的过程为随机梯度下降，关于随机梯度下降算法等可以参考神经网络和深度学习的博客。

上面就是其结构，说得非常简单，但是上面有几个需要提的问题:
1.知道了所有节点和边的特征，如何计算得到节点的状态:
在状态转换方程中，输入中有变量与该节点相邻的节点的状态，输出为该节点的状态，这个方程的求解需要使用一个理论，就是上面提到的Banach不动点理论。
2.训练的过程中有两个方程，每个方程都需要训练不同的参数，如何同时训练两个方程:
由于原始GNN网络结构的特殊性，有特殊的计算方法。

下面是程序流程图:
![](原始GNN网络程序流程图1.png)
![](原始GNN网络程序流程图2.png)

直接看流程图可以发现流程依然简单:
1.初始化所有方程中的权重，也就是所有的参数
2.通过初始化的参数计算出每个节点的状态，其中使用了FORWARD(w)算法。
3.然后，使用随机梯度下降算法不断迭代，计算出最终学习之后的参数数值。其中使用了BACKWARD()算法。

其中，FORWARD算法就是利用不动点理论，对方程进行迭代处理，就会不断接近节点的状态。
BACKWARD算法依然是利用了状态全局收敛，利用不动点理论，然后进行迭代，求解二次代价函数对参数的偏导。

上面就是对其理论的简单介绍。

下面，就是代码实现:
1.原始GNN网络的模型验证:
自己编写的原始GNN网络，其中的数据为自己随机建立的数据，用来了解原始GNN网络的结构与原理，其内容为随机将18个点分为3类，每一类有6个点，用不同的颜色表示，通过部分节点进行标注训练对其他节点的类别进行分类预测。
![](原始GNN网络验证模型.png)

其输出结果为:

