---
title: 原始GNN和GCN网络详解
date: 2020-09-10 00:12:07
tags:
- 原始GNN
- GCN
---
# 0.概述
之前的博客中学习了神经网络，但是，神经网络只能处理简单的类似于表格的数据，其中没有各个数据之间的联系，例如图片数据等。
但是在实际中，我们需要表示各个数据之间的联系，所以我们引入了图结构，图中的节点表示数据，节点之间的线表示各个数据之间的关系，图结构能够表达出比表数据更多的内容。
下面是图结构数据的图像表示:
![](图结构数据图像.png)

这部分内容为:原始GNN网络和GCN网络



# 1.原始GNN网络

为了方便理解，我们需要建立如下的变量:
![](原始GNN网络变量.png)
其中，获取的数据中有每个节点的特征向量

简单解释一下原始GNN网络的结构。
1.对每个节点建立一个状态变量，每个节点的状态变量都取决于该节点的特征、和该节点相连接的边的特征、和该节点相连节点的特征、和该节点相邻节点的状态，对此建立一个状态转换方程
2.对每一个节点建立一个输出变量，每个节点的输出变量取决于该节点的状态、该节点的特征，对此建立一个输出方程
3.建立一个二次损失函数，使得其最小

上面两个方程中:
输出方程为线性方程，其中的系数都是需要训练的参数，初始时使用符合正态分布的随机数，
状态转换方程比较复杂，因为状态变量需要有解，也就是能够全局收敛，需要满足Banach不动点理论，为此，状态转换方程需要满足一定的条件，具体的条件下面继续说明。
在之后的训练中会对这些参数进行训练，使得二次损失函数越来越小，输出越来越接近我们期望的输出。

训练的过程为随机梯度下降，关于随机梯度下降算法等可以参考神经网络和深度学习的博客。

上面就是其结构，说得非常简单，但是上面有几个需要提的问题:
1.知道了所有节点和边的特征，如何计算得到节点的状态:
在状态转换方程中，输入中有变量与该节点相邻的节点的状态，输出为该节点的状态，这个方程的求解需要使用一个理论，就是上面提到的Banach不动点理论。
2.训练的过程中有两个方程，每个方程都需要训练不同的参数，如何同时训练两个方程:
由于原始GNN网络结构的特殊性，有特殊的计算方法。

下面是程序流程图:
![](原始GNN网络程序流程图1.png)
![](原始GNN网络程序流程图2.png)

直接看流程图可以发现流程依然简单:
1.初始化所有方程中的权重，也就是所有的参数
2.通过初始化的参数计算出每个节点的状态，其中使用了FORWARD(w)算法。
3.然后，使用随机梯度下降算法不断迭代，计算出最终学习之后的参数数值。其中使用了BACKWARD()算法。

其中，FORWARD算法就是利用不动点理论，对方程进行迭代处理，就会不断接近节点的状态。
BACKWARD算法依然是利用了状态全局收敛，利用不动点理论，然后进行迭代，求解二次代价函数对参数的偏导。

上面就是对其理论的简单介绍。

下面，就是代码实现:
1.原始GNN网络的模型验证:
自己编写的原始GNN网络，其中的数据为自己随机建立的数据，用来了解原始GNN网络的结构与原理，其内容为随机将18个点分为3类，每一类有6个点，用不同的颜色表示，通过部分节点进行标注训练对其他节点的类别进行分类预测。
![](原始GNN网络验证模型.png)

其输出结果为:
![](原始GNN网络验证模型输出1.png)
![](原始GNN网络验证模型输出2.png)

2.原始GNN网络Core数据集分类:
这个实现更加接近实际应用情况，采用Core数据集，对其中的内容进行分类。
Core数据集中为机器学习领域的论文，其中被分为了七个类别，其中，一共有2708篇论文，有1433个关键词作为特征，论文之间的引用关系作为边有5429条边。

# 2.GCN网络
首先，GCN网络需要一些前提知识:
![](GCN邻接矩阵.png)
上面的度矩阵表示每个节点相邻节点的数目，邻接矩阵表示各个节点之间的连接关系，下面简单介绍一下GCN公式的原理。
![](GCN公式简单推导1.png)
![](GCN公式简单推导2.png)
第一个公式类似于之前博客中的神经网络，只是输入中增加了邻接矩阵。
第二个公式就是优化后的GCN网络:
1.将邻接矩阵A增加一个单位矩阵得到矩阵A波浪，这样避免在计算当前节点时候，忽略当前节点的特征，经过实验表明，这样可以增加网络的精度。
2.增加归一化步骤，上面的D波浪是A波浪的度矩阵，进行归一化可以避免很多意外的问题。
下面是输出结果:
![](GCN输出结果.png)

代码实现:
1.GCN根据MR数据进行情感分类:
直接输入了电影的评论文本数据，分别有两个标签:积极和消极
节点为评论和词语
边为：评论和词语、词语和词语
对于边的权重如下图所示:
![](GCN网络MR数据边权重公式.png)

首先是词语和评论之间的权重:TF-IDF
TF:Term Frequency
表示某个词语在段落中出现的次数，公式如下。
![](TF公式.png)
n(i,j) 词i在文档j中出现的次数
分母为文档j中所有词出现的次数之和

IDF:Inverse Document Frequency
反文档频率:包含词语的段落数量的反比。
它的含义为:包含词语的段落越少，则IDF越大，说明这个词语有很强的区别能力，表现词语的重要性大小，公式如下。
![](IDF公式.png)
|D| 语料库中doc总数
分母为包含词条ti的doc的数目，避免为0，要加1

那么TF-IDF为TF×IDF。

然后是词语与词语之间的权重PMI:
用来表示两个词语之间的语义相关性，<0表示相关性弱或没有相关性，>0表示相关性强，公式如下图。
![](PMI公式.png)
上面的公式中:
#W(i)包含词i的滑窗数目
#W(i,j)包含词i和词j的滑窗数目
#W表示滑窗总数


# 3.原始GNN网络和GCN网络对比

## 3.1 数据:
这两个算法都是在一个固定的图上面进行训练，如果增加新的节点需要重新进行训练

## 3.2 算法:
(1)GNN中节点的状态，需要与它相连的节点的状态，从而保存了边的信息，而这两个都是未知的，所以，利用了不动点原理，通过迭代的方法，我们可以计算出节点的状态。
(2)GCN中每一层的状态由上一层的状态所决定，和当前层的状态没有关系。通过邻接矩阵来保存边的信息。
从而:
(1)GCN省去了不动点迭代的过程，可以产生多层的结构。
(2)为了保证公式能够有解，GNN需要引入额外的参数进行学习，而GCN的参数就仅仅是上面公式里面的权重。参数数量的减少使得GCN的训练效果更加快速。

