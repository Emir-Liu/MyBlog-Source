---
title: 'Pytorch自然语言处理'
date: 2020-07-27 08:33:51
tags:
- pytorch
- nlp
---
# 0.简介
资料来源：
PyTorch自然语言处理（Natural Language Processing with PyTorch）

主要内容:自然语言处理NLP和深度学习，公式部分没有详细的分析。

章节：

1.基础介绍
2.传统NLP快速回顾
3.神经网络基础组建
4.自然语言处理前馈网络Feed-Forward Networks
5.Embedding Words and Types
6.自然语言处理Sequence Modeling
7.自然语言处理Sequence Modeling
8.用于自然语言处理的高级Sequence Modeling
9.经典，前沿和后续步骤

# 1.基础介绍

## 1.1 有监督学习
![](有监督学习.png)
如上图所示:
Data:输入数据集中包含下面两个
Observation:观察对象，用x来表示
Targets:与观察对象相对应的标签，用y来表示
Model:数学表达式或者是一个函数，它接收x观察值，并预测目标的标签值。
Parameters:参数化模型中的权重，使用符号为w
Prediction:预测，是模型在给定观察值的情况下，所猜测的目标的标签值，我们通常在其后面加上hat
Loss Function:
比较预测值和观察对象之间的差距，返回一个标量。损失值越低，模型对目标的预测效果越好，用L来表示损失函数。

结合上面的概念，用数学来表示:
一个数据集 D={X[i],y[i]}，i=1..n，有n个例子。
给定这个数据集，我们想要学习一个权值w参数化的函数(模型)f。换言之，我们对f的结构做一个假设，给定这个结构，权值w的学习值将充分表征模型。
对于一个给定的输入X,模型预测y_hat作为目标: y_hat = f(X;W) 
在监督学习中，对于训练例子，我们知道观察的真正目标标签y。这个实例的损失将为 L(y, y_hat) 。然后，监督学习就变成了一个寻找最优参数/权值w的过程，从而使所有n个例子的累积损失最小化。

## 1.2 (随机)梯度下降法及反向传播
利用(随机)梯度下降法进行训练，监督学习的目标是为给定的数据集选择参数值，使损失函数最小化。换句话说，这等价于在方程中求最小值。
我们知道梯度下降法是一种常见的求最小值的方法。回忆一下，在传统的梯度下降法中，我们对参数的一些初值进行猜测，并迭代更新这些参数，直到目标函数(损失函数)的计算值低于可接受阈值(即收敛准则)。
对于大型数据集，由于内存限制，在整个数据集上实现传统的梯度下降通常是不可能的，而且由于计算开销，速度非常慢。相反，通常采用一种近似的梯度下降称为随机梯度下降(SGD)。在随机情况下，数据点或数据点的子集是随机选择的，并计算该子集的梯度。当使用单个数据点时，这种方法称为纯SGD，当使用(多个)数据点的子集时，我们将其称为小型批处理SGD。
通常情况下，“纯”和“小型批处理”这两个词在根据上下文变得清晰时就会被删除。在实际应用中，很少使用纯SGD，因为它会由于有噪声的更新而导致非常慢的收敛。一般SGD算法有不同的变体，都是为了更快的收敛。在后面的章节中，我们将探讨这些变体中的一些，以及如何使用渐变来更新参数。这种迭代更新参数的过程称为反向传播。反向传播的每个步骤(又名epoch)由向前传递和向后传递组成。向前传递用参数的当前值计算输入并计算损失函数。反向传递使用损失梯度更新参数。

## 1.3 观察对象和标签的编码
![](观察对象和标签的编码.png)
如上图所示，我们需要用数字来表示观察值(文本)，以便与机器学习算法一起使用。
表示文本的一种简单的方法就是用数字向量来表示。我们使用其中一个简单的方法:

### 1.3.1 One-Hot表示方式
one-hot表示从一个0向量开始，如果单词出现在句子或者文档中，则将向量中相应的条目设置为1,例如下面的两句话:
'''bash
Time flies like an arrow
Fruit flies like a banana
'''
对句子进行标记，忽略标点符号，然后将所有单词用小写字母来表示，就会得到一个大小为8的词汇表{time, fruit, flies, like, a, an, arrow, banana}。所以，我们可以用一个8维的one-hot向量来表示每一个单词，我们使用l[w]来表示单词w的one-hot表示。

对于短语、句子或者文档，其one-hot表示仅仅是它的组成词语的one-hot表示的逻辑或。

例如，短语'like a banana'的one-hot表示就是一个3×8的矩阵。
通常，会看到折叠形式的或二进制编码，其文本由和词汇表相同的向量表示，用0或1表示缺失或者存在。‘like a banana’的二进制编码是:[0,0,0,1,1,0,0,1]。备注，折叠one-hot是一个向量中有多个1的one-hot
![](LikeABanana.png)

注意，可能对flies的两种意思弄混了，但是语言中充满了这种歧义，但是我们可以通过简单的假设来构建方案。使其学习特定意义的表示，但这个要之后再说。

本文章通常使用one-hot表示，但是在NLP中还有其他的表示方法。
### 1.3.2 TF表示
句子的TF表示仅仅是句子中词的one-hot总和。
使用前面的one-hot编码，“Fruit flies like time flies a fruit”这句话具有以下TF表示:[1,2,2,1,1,1,0,0]。
每一个数字是句子中出现相应单词的次数

### 1.3.3 TF-IDF表示
一个文件中可能会出现相同的词语很多次。
例如专利文件中，里面claim,system,method等单词会经常出现很多次。
TF表示方式对更频繁的词进行加权，但是，上面的词语不会增加我们对文章内容的理解，相反，如果类似tetrafluoroethylene这样的词语出现的频率比较低，但是很可能表明了专利文件的性质，所以希望给予它更加大的权重，反文档频率是一种启发式的算法，可以精确地做到这一点。

TF-IDF会降低常见词语的权重，而增加不常见词语的权重。
IDF(w)的定义是:
n[w]是包含单词w的文档数量，N是文档总数。TF-IDF=TF(w)*IDF(w)
假如所有文档中都有这个词语，那么数值为0,当一个词语很少出现，可能只出现在一个文档中，那么IDF就是最大值。

在深度学习中，很少看到使用像TF-IDF这样的启发式表示对输入进行编码，因为目标是学习一种表示。
通常，我们从一个使用整数索引的one-hot编码和一个特殊的“embedding lookup”层开始构建神经网络的输入。



# 2.传统NLP快速回顾
# 3.神经网络基础组建
# 4.自然语言处理前馈网络Feed-Forward Networks
# 5.Embedding Words and Types
# 6.自然语言处理Sequence Modeling
# 7.自然语言处理Sequence Modeling
# 8.用于自然语言处理的高级Sequence Modeling
# 9.经典，前沿和后续步骤
