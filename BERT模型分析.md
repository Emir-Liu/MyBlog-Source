---
title: BERT模型分析
date: 2020-08-10 10:05:49
tags:
---
# 0.简介
BERT Bidirectional Encoder Representations from Transformers
一种从Transformers模型得来的双向编码表征模型
下面通过理论和实践两部分来解释BERT模型
1.理论：
1.1 为什么需要BERT
1.2 它背后的核心思想是什么
1.3 它是如何工作的

2.实践
2.1 我们什么时候可以使用它并且对其进行微调
# 1.理论
## 1.1 为什么需要BERT
NLP的最大挑战之一是缺乏足够的培训数据。总体而言，有大量文本数据可用，但是如果我们要创建特定于任务的数据集，则需要将该堆拆分为许多不同的类型。而当我们这样做时，我们最终仅得到数千或数十万个人工标记的训练样本。但是，为了表现良好，基于深度学习的NLP模型需要大量数据-在数百万或数十亿的带注释的训练样本上进行训练时，他们看到了重大改进。为了解决上面的数据问题，研究人员开发了多种技术，可在网络上使用大量未注释的文本来训练通用语言表示模型（这被称为预训练）。然后，可以在较小的用于特定任务的数据集上微调这些通用的预训练模型，例如，在处理诸如问题回答和情感分析之类的问题。与从头开始对较小的特定于任务的数据集进行训练相比，此方法可显着提高准确性。BERT是用于NLP预训练的最新的技术。它在深度学习社区引起了轰动，因为它在各种NLP任务（例如问题解答）中都提供了最先进的结果。

关于BERT的最好的部分是可以免费下载和使用-我们可以使用BERT模型从文本数据中提取高质量的语言特征，也可以根据特定任务（例如情感分析）微调这些模型并使用我们自己的数据回答问题，以产生最先进的预测。
## 1.2 它背后的核心思想是什么
语言建模的真正意义是什么？语言模型试图解决哪些问题？基本上，他们的任务是根据上下文“填补空白”。例如，给定

“那个女人去商店买了_____鞋。”
'The woman went to the store and bought a __ of shoes'

语言模型可能会说20%是'cart'，80%是'pair'

在BERT模型出现之前，语言模型会在训练期间从左到右或这将从左到右和从右到左的结合来查看此文本序列。这种单向方法很适合生成句子-我们可以预测下一个单词，将其附加到序列中，然后预测下一个单词的下一个单词，直到获得完整的句子。

现在使用BERT模型，这是一种经过双向训练的语言模型（这也是其关键的技术创新）。这意味着与单向语言模型相比，我们现在可以对语言上下文和流程有更深刻的了解。

BERT并没有预测序列中的下一个单词，而是使用一种称为Masked LM（MLM）的新颖技术：它 随机屏蔽句子中的单词，然后尝试预测它们。掩蔽Mask意味着该模型从两个方向看，并且它使用句子的整个上下文（包括左边和右边的语境）来预测被掩盖的单词。与以前的语言模型不同，它同时考虑了前一个和下一个tokens令牌  。LSTM模型将从左到右和从右到左相结合，并没有同时考虑这两个方向。（不过，说BERT是无方向性的可能更准确。）

额外提出一个问题，为什么这个无方向性的方法这么有效呢？
预训练语言表示分为context-free(和上下文无关)和context-based(基于上下文).
context-based表示可以是单向或者双向。
context-free表示,例如word2vec产生了一个简单的词嵌入(数字向量)表示词汇表中的每一个词语。
例如bank将会有相同的context-free表示在'bank account'和'bank of river'中。

另一方面，基于上下文的模型建立代表每一个词语的表示是基于句子中的其他词语。
例如:I accessed the bank account
在单向网络模型中，词语bank是基于I accessed the bank 而不是 account
但是，在BERT中，bank是使用了其上下文 I accessed the ... account
它从深度神经网络的最底层开始，使其深度双向化。

而且，BERT是基于Transformer model architecture,而不是LSTM，我们很快就会讨论BERT的细节，但是总体而言:



## 1.3 它是如何工作的

# 2.实践
## 2.1 我们什么时候可以使用它并且对其进行微调
