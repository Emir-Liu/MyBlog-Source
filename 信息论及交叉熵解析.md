---
title: 信息论及交叉熵解析
date: 2020-09-09 01:50:42
tags:
- 信息论
- 交叉熵
---
# 0.概述
交叉熵(Cross Entropy)是Shannon信息论中的重要概念，用于度量两个概率之间的分布差异。
这个在机器学习中常常作为成本函数出现，为了了解交叉熵顺便了解了信息论中的内容。这边文章主要对信息论及其中经常使用的交叉熵函数进行分析。

# 1.香农熵
香农熵在1948年提出，基本概念就是:一条信息的信息量大小和它的不确定性有直接的关系。这解决了对信息的量化度量问题。
一个变量的不确定性越大，那么熵越大，那么将其搞清楚所需要的信息量越大。
其公式为:
![](香农熵的定义.png)
上面的描述比较抽象，下面举一个具体的例子:
在硬币抛掷实验中，1bit信息代表单独抛掷一个硬币的两个可能的结果。
假如连续三次投掷硬币，一共有2^3=8种可能的结果，每种结果的可能性为0.5^3=0.125。
所以这次实验的自信息为-log_2(0.125) = 3,也就是需要3bit来表示其信息，
这样我们就能够看出，小概率对应着较高的自信息，如下图所示。




这个例子中，我们到现在一直讨论的都是自信息。在正常的硬币实验中，自信息实际上都等于香农熵，因为所有的结果都是等概率出现的。通常，香农熵是变量的所有可能结果的自信息的期望值。
![](香农熵推导.png)
上面对数的底数为b，一开始为2,但是通常都使用10或者e方便计算，影响不大，因为底数不同仅仅是前面的系数不同。

# 2.交叉熵
交叉熵是用来比较两个概率分布p和q的数学工具，和香农熵类似，交叉熵为在概率p的情况下，q的自信息-log(q)
![]()

